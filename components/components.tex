\section{Core components}

\input{components/debug.tex}
\input{components/logging.tex}
\input{components/at_scale.tex}
\input{components/vis.tex}
\input{components/image.tex}

\subsection{Catalog visualization tools}

\assign{Lauren}

Derived from \S\ref{sec:design:debug}.

For visualizing bigger-than-memory catalogs. May include e.g. the capability
to spin up Dask clusters on demand, combined with
Holoviews/Datashader/whatever. Somebody who knows about this stuff needs to
write a summary...

\subsection{Provenance}

\assign{Hsin-Fang}

Derived from \S\ref{sec:design:debug}.

This section should note:

\begin{itemize}

  \item{That provenance is an immediate issue impacting QA work, so a solution
  is a priority;}

  \item{Some requirements as to the granularity at which provenance tracking
  is necessary for QA.}

\end{itemize}

\subsection{Documentation content updates}
\label{sec:comp:doc}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Clearer guidance on unit tests.}
  \item{Clearer guidance on code review, with requirements for test coverage
  etc.}

\end{itemize}

\subsection{Testing for documentation}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Examples.}

\end{itemize}

\subsection{CI system updates}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Test coverage.}
  \item{Tighter control of the environment.}
  \item{Better notifications.}
  \item{Better descriptions of which jobs do what.}
  \item{Clear description of what Developers are required to do before merging
  to master (see also \S\ref{sec:comp:doc}).}

\end{itemize}


\subsection{Metrics Dashboard / SQuaSH}

Derived from \S\ref{sec:design:test}.

\assign{Angelo}

To date, SQuaSH has been used to follow a subset of KPMs computed by validade_drp for tracking performance regressions due to pipeline changes by regularly reprocessing test datasets in Jenkins/CI.

The following recommendations would enhance SQuaSH capabilities for DM developers.

\begin{recommendation}
SQuaSH should be used by developers for tracking metrics on their particular projects.
\end{recommendation}

Developers can instrument their science pipeline Tasks using \texttt{lsst.verify} and create new verification packages to be tracked in SQuaSH (see e.g. \texttt{jointcal}). It would be interesting to send results to SQuaSH when testing development branches, so that developers can compare the new metric values with the previous values \textit{before} merging to master. Any metric defined in \texttt{lsst.verify.metrics} should be uploaded to SQuaSH, for example, computational metrics like code execution time.

\begin{recommendation}
SQuaSH should provide automated notification of regressions.
\end{recommendation}

Metric specifications in \texttt{lsst.verify} include thresholds that can be used to automatically detect and notify regressions. The notifications can be presented to developers by Slack, for example.

\begin{recommendation}
SQuaSH should provide a metric summary display.
\end{recommendation}

Verification packages might have specialized visualizations for displaying metric summary information in addition to the current time series plot. DM developers should be able to extend SQuaSH by creating new visualizations following developer documentation provided in the SQuaSH Documentation (https://squash.lsst.io/)

\begin{recommendation}
SQuaSH should support the LDF execution environment in addition to Jenkins/CI.
\end{recommendation}

Pipeline runs on larger datasets (e.g. HSC RC2 weekly reprocessing) require more computation than can be provided in the Jenkins/CI environment. SQuaSH should be flexible to support other environments like the LDF environment.

\begin{recommendation}
SQuaSH should be able to store and display metric values per DataIds (e.g. CCD, visit, patch, tract, filter).
\end{recommendation}

Pipeline runs on larger datasets (e.g. HSC RC2 weekly reprocessing) also require to store and display metric values per DataId as opposite to the entire dataset, as it is currently with the test datasets in CI. The ability to identify metric values per filter name or spatially by CCD in a visit or per patch in a tract, would enhance SQuaSH display and monitoring capabilities, turning SQuaSH or its successor into a richer metric dashboard as described in \S\ref{sec:design:drill_down}.


\subsection{Standard format dataset package}
\label{sec:comp:lfs-dataset}

Derived from \S\ref{sec:design:test}.

\assign{Hsin-Fang}

\subsection{Standard test package design}

Derived from \S\ref{sec:design:test}.

\assign{Hsin-Fang}

Should address the union of lsst\_dm\_stack\_demo, ci\_hsc, validate\_drp use
cases.

\subsection{Updates to guidelines for GPFS-based dataset storage}
\label{sec:comp:gpfs-dataset}
