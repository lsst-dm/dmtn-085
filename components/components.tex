\section{Core components}

\subsection{Updated pipeline debugging system}

\assign{Simon}

Derived from \S\ref{sec:design:debug}.

i.e. redesigned \texttt{lsstDebug}.

\subsection{Logging}

\assign{Simon}

Derived from \S\ref{sec:design:debug}.

\subsection{Capability for developers to run pipelines at scale}

\assign{Lauren}

Derived from \S\ref{sec:design:debug}.

\subsection{Guidance on visualization}

\assign{Lauren}

Derived from \S\ref{sec:design:debug}.

We're requesting a set of guidelines for developers here, not a new framework
--- but that's still a concrete deliverable (it's just documentation, rather
than code). We might suggest that these guidelines be developed by a new WG,
per Simon's
suggestion\footnote{\url{https://confluence.lsstcorp.org/display/DM/Pipeline+Debugging+Design}}.

\subsection{Image viewer}

\assign{Trey}

Derived from \S\ref{sec:design:debug}.

As of 2018-06-12 we haven't converged on a solid recommendation here.

Key considerations:

\begin{itemize}

  \item{Firefly is the annointed solution being provided by DM to external
  stakeholders (commissioning, operations, etc). It feels right to everybody
  that we should be dogfooding it, and also benefitting from development being
  carried out for those stakeholders.}

  \item{Currently, Firefly is unappealing to developers (primarily, I think,
  because of slowness of user interface, and perhaps also due to installation
  issues). Can we resolve these issues?}

  \item{We'd want to support visualization in a number of different
  environments, for e.g.:

    \begin{itemize}

      \item{Inside a Jupyter notebook;}
      \item{As a standalone tool, \`a la DS9;}
      \item{Embedded in a \gls{dashboard}, \`a la JS9, Aladin-Lite, etc.}

    \end{itemize}
  }

  \item{Do we lose flexibility by mandating the use of a backend-agnostic API
  (\texttt{afwDisplay}) rather than going ``all-in'' on e.g. a custom Firefly
  interface?}

  \item{We'll need to do full focal plane visualization, which none(?) of the
  current tools support well.}

\end{itemize}

Options include:

\begin{itemize}

  \item{Do nothing; continue as we are, which means most people will use DS9
  and a few will drift to Firefly as commissioning ramps up.}

  \item{Issue some sort of edict that pipelines developers have to use
  Firefly.}

  \item{Encourage the use of some other tool (Ginga?) instead of or as well as
  some of the above.}

  \item{Probably others.}

\end{itemize}

Sounds like we need somebody from the QAWG to actually write some requirements
--- or a wishlist set of features we want --- here.

\subsection{Catalog visualization tools}

\assign{Lauren}

Derived from \S\ref{sec:design:debug}.

For visualizing bigger-than-memory catalogs. May include e.g. the capability
to spin up Dask clusters on demand, combined with
Holoviews/Datashader/whatever. Somebody who knows about this stuff needs to
write a summary...

\subsection{Provenance}

\assign{Hsin-Fang}

Derived from \S\ref{sec:design:debug}.

This section should note:

\begin{itemize}

  \item{That provenance is an immediate issue impacting QA work, so a solution
  is a priority;}

  \item{Some requirements as to the granularity at which provenance tracking
  is necessary for QA.}

\end{itemize}

\subsection{Documentation content updates}
\label{sec:comp:doc}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Clearer guidance on unit tests.}
  \item{Clearer guidance on code review, with requirements for test coverage
  etc.}

\end{itemize}

\subsection{Testing for documentation}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Examples.}

\end{itemize}

\subsection{CI system updates}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Test coverage.}
  \item{Tighter control of the environment.}
  \item{Better notifications.}
  \item{Better descriptions of which jobs do what.}
  \item{Clear description of what Developers are required to do before merging
  to master (see also \S\ref{sec:comp:doc}).}

\end{itemize}


\subsection{Metrics Dashboard / SQuaSH}

Derived from \S\ref{sec:design:test}.

\assign{Angelo}

To date, SQuaSH has been used to follow a subset of KPMs computed by validade_drp for tracking performance regressions due to pipeline changes by regularly reprocessing test datasets in Jenkins/CI.

The following recommendations would enhance SQuaSH capabilities for DM developers.

\begin{recommendation}
SQuaSH should be used by developers for tracking metrics on their particular projects.
\end{recommendation}

Developers can instrument their science pipeline Tasks using \texttt{lsst.verify} and create new verification packages to be tracked in SQuaSH (see e.g. \texttt{jointcal}). It would be interesting to send results to SQuaSH when testing development branches, so that developers can compare the new metric values with the previous values \textit{before} merging to master. Any metric defined in \texttt{lsst.verify.metrics} should be uploaded to SQuaSH, for example, computational metrics like code execution time.

\begin{recommendation}
SQuaSH should provide automated notification of regressions.
\end{recommendation}

Metric specifications in \texttt{lsst.verify} include thresholds that can be used to automatically detect and notify regressions. The notifications can be presented to developers by Slack, for example.

\begin{recommendation}
SQuaSH should provide a metric summary display.
\end{recommendation}

Verification packages might have specialized visualizations for displaying metric summary information in addition to the current time series plot. DM developers should be able to extend SQuaSH by creating new visualizations following developer documentation provided in the SQuaSH Documentation (https://squash.lsst.io/)

\begin{recommendation}
SQuaSH should support the LDF execution environment in addition to Jenkins/CI.
\end{recommendation}

Pipeline runs on larger datasets (e.g. HSC RC2 weekly reprocessing) require more computation than can be provided in the Jenkins/CI environment. SQuaSH should be flexible to support other environments like the LDF environment.

\begin{recommendation}
SQuaSH should be able to store and display metric values per DataIds (e.g. CCD, visit, patch, tract, filter).
\end{recommendation}

Pipeline runs on larger datasets (e.g. HSC RC2 weekly reprocessing) also require to store and display metric values per DataId as opposite to the entire dataset, as it is currently with the test datasets in CI. The ability to identify metric values per filter name or spatially by CCD in a visit or per patch in a tract, would enhance SQuaSH display and monitoring capabilities, turning SQuaSH or its successor into a richer metric dashboard as described in \S\ref{sec:design:drill_down}.


\subsection{Standard format dataset package}
\label{sec:comp:lfs-dataset}

Derived from \S\ref{sec:design:test}.

\assign{Hsin-Fang}

\subsection{Standard test package design}

Derived from \S\ref{sec:design:test}.

\assign{Hsin-Fang}

Should address the union of lsst\_dm\_stack\_demo, ci\_hsc, validate\_drp use
cases.

\subsection{Updates to guidelines for GPFS-based dataset storage}
\label{sec:comp:gpfs-dataset}
