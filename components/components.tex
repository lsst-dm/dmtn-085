\section{Core components}

\input{components/debug.tex}
\input{components/logging.tex}
\input{components/at_scale.tex}
\input{components/vis.tex}
\input{components/image.tex}

\subsection{Catalog visualization tools}

\assign{Lauren}

Derived from \S\ref{sec:design:debug}.

For visualizing bigger-than-memory catalogs. May include e.g. the capability
to spin up Dask clusters on demand, combined with
Holoviews/Datashader/whatever. Somebody who knows about this stuff needs to
write a summary...

\subsection{Provenance}

\assign{Hsin-Fang}

The QAWG recognizes the importance of provenance and the
implementation of the provenance system will impact QA work
significantly.  We recommend high priority to finalize the design
and implementation of the provenance system.

The QAWG believes that the requirements on provenance tracking are
adequate as described in the Data Management Middleware Requirements
(\citeds{LDM-556}) and Data Management Data Backbone Services
Requirements (\citeds{LDM-635}). QA use cases provide no further
requirements.

With the current design of the Gen 3 Middleware, each dataset
can be linked to provenance information such as input datasets,
pipeline definition, configurations, and software version.
QC metric values will be Butler datasets so they will be associated
with specific data IDs, and their provenance can be traced like
other datasets.  The Gen3 Butler/SuperTask generated provenance is
the primary source of provenance in the QA use cases as the
Butler/SuperTask framework is responsible of producing provenance
in the Gen 3 Middleware design.  For production runs, the Data
Backbone Services may store additional provenance, for example from
the Batch Production Services, besides the Butler/SuperTask generated
provenance.

Regarding provenance of the database records, the QAWG does not
place a strong requirement on per-source provenance.  The current
design is that each database record in the production database is
either ingested from a file, of which the full provenance is traced,
or from an uniquely identifiable execution.  This design does not
directly provide detailed per-source provenance, such as what exact
input images acually contribute to the measurement of a particular
source, but the full inputs that can contribute to the source.  The
provenance tracking of synthetic sources is also unclear.  The QAWG
thinks most low-level information can be uncovered through the drill
down system (\S\ref{sec:design:drill}).  Diagnostic information can
also be computed in the pipeline codes and stored as additional
columns.  We recommend the tooling to investigate the composition
of coadds be made in the drill down system and does not put this
requirement in the provenance system.

The QAWG notes that some high level aggregate data products that
are derived from provenance data can be useful in QA work.  For
example, the number of images that contribute to each coadd patch
can be obtained from provenance data.  However, the QAWG are not
immediately convinced that it's worth spending significant time
investigating what aggregate products need to be considered.

Derived from \S\ref{sec:design:debug}.

This section should note:

\begin{itemize}

  \item{That provenance is an immediate issue impacting QA work, so a solution
  is a priority;}

  \item{Some requirements as to the granularity at which provenance tracking
  is necessary for QA.}

\end{itemize}

\subsection{Documentation content updates}
\label{sec:comp:doc}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Clearer guidance on unit tests.}
  \item{Clearer guidance on code review, with requirements for test coverage
  etc.}

\end{itemize}

\subsection{Testing for documentation}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Examples.}

\end{itemize}

\subsection{CI system updates}
\label{sec:comp:ci}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Test coverage.}
  \item{Tighter control of the environment.}
  \item{Better notifications.}
  \item{Better descriptions of which jobs do what.}
  \item{Clear description of what Developers are required to do before merging
  to master (see also \S\ref{sec:comp:doc}).}

\end{itemize}


\subsection{Metrics Dashboard / SQuaSH}

Derived from \S\ref{sec:design:test}.

\assign{Angelo}

Broadly as current SQuaSH, but to track:

\begin{itemize}

  \item{Code execution time.}
  \item{Test coverage?}
  \item{Notifications of regressions.}

\end{itemize}

\subsection{Standard format dataset package}
\label{sec:comp:dataset}

Derived from \S\ref{sec:design:test}.

\assign{Hsin-Fang}

The key considerations and motivations to standardize test datasets include:

\begin{itemize}

\item{Developers would like low-friction access to test datasets.
A central location where developers can look up what has been curated
is desired.}

\item{Developers would like datasets at multiple scales and
representative of various data quality and observing conditions.
The properties of these datasets much be well understood.}

\item{Besides raw (unprocessed) data, intermediate and final data
products from various stages of pipeline processing are wanted.
They facilitiate testing of algorithms which are only relevant to
later parts of the pipeline or analysis codes without the need of
regenerating the products.}

\item{Datasets require continuing maintenance. Data products based
on a recent release are usually wanted.}

\end{itemize}


The standard format of a dataset package is a ready-to-use Butler
repository and follows the format of a Butler repository as defined
in its corresponding obs package.  The format is configurable by
design, however, it is tied to the codes in the stack, so can change
from a software stack version to another.  Besides implementations
in the obs packages and Butler, other evolvement in the software
stack, such as handling of calibration data and reference catalog,
can also make a once-working repository incompatible.  Therefore,
maintenance is occassionally needed to ensure the usability of a
dataset package.  The QAWG recommends having a per-dataset product
owner.  Product owners are responsible for ensuring that datasets
are well described and compatible with recent stack versions.  The
owner of a dataset can typically be the team with immediate use
cases and knowledge of its camera package.

The Obs Pkg WG (\jira{RFC-393}) is charged to re-design and refactor
the obs packages for maintainability and extensibility. We suggest
the Obs Pkg WG take into considerations in their design to mitigate
the close tie between a Butler repository and its obs package
implmentations, as well as adopt a common structure across different
cameras when possible.  After the refactoring, the obs packages
shall rarely change so the dataset format will be more stable.  The
QAWG recommends prioritise the Obs Pkg WG.

In some cases, a dataset pacakge may contain additional data not
in the format of a Butler repository. We recommend following the
format as described in DM Developer Guide Common Dataset Organization
and Policy \footnote{\url{https://developer.lsst.io/services/datasets.html}}
and update the policy as needed.

As for the storage of test datasets, we consider any test dataset
package being either small or large, based on its size and use cases.

\begin{itemize}
  \item \textbf{Storage of small datasets}

  We consider small datasets as those smaller than around 100 GB
  and comfortably operatable as a Git LFS repository. They are
  carefully selected to meet specific use cases. The use cases of
  small datasets include

  \begin{itemize}
    \item{as input test data in CI jobs in the DM Jenkins system
          (\S\ref{sec:comp:ci});}
    \item{as example data in documentations, demos, and tutorials.}
  \end{itemize}

  To meet their needs, we recommend them

  \begin{itemize}
    \item{managed as eups packages;}
    \item{stored as Git LFS repositories as needed;}
    \item{tagged their versions with the software releases;}
    \item{made publicly available on GitHub;}
    \item{documented clearly its use cases and named product owner for each dataset;}
  \end{itemize}

  \item \textbf{Storage of large datasets}

  We consider large datasets as those larger than around 100 GB and
  hence transferring over network takes longer than an hour typically.
  They can contain edge cases that have not been identified to form
  specific small test datasets, or for use cases in which data
  volume is important.  We recommend them

  \begin{itemize}
    \item{made available on LSST Development Machines (currently on GPFS);}
    \item{protected under a disaster recovery policy;}
    \item{shared by team members;}
    \item{documented clearly its use cases and named product owner for each dataset;}
  \end{itemize}

\end{itemize}



\subsection{Standard test package design}

Derived from \S\ref{sec:design:test}.

\assign{Hsin-Fang}

Should address the union of lsst\_dm\_stack\_demo, ci\_hsc, validate\_drp use
cases.

\subsection{Updates to guidelines for GPFS-based dataset storage}
\label{sec:comp:gpfs-dataset}
