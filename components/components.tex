\section{Core components}

\input{components/debug.tex}
\input{components/logging.tex}
\input{components/at_scale.tex}
\input{components/vis.tex}
\input{components/image.tex}

\subsection{Catalog visualization tools}

\assign{Lauren}

Derived from \S\ref{sec:design:debug}.

For visualizing bigger-than-memory catalogs. May include e.g. the capability
to spin up Dask clusters on demand, combined with
Holoviews/Datashader/whatever. Somebody who knows about this stuff needs to
write a summary...

\subsection{Provenance}

\assign{Hsin-Fang}

The QAWG recognizes the importance of provenance and the
implementation of the provenance system will impact QA work
significantly.  We recommend high priority to finalize the design
and implementation of the provenance system.

The QAWG believes that the requirements on provenance tracking are
adequate as described in the Data Management Middleware Requirements
(\citeds{LDM-556}) and Data Management Data Backbone Services
Requirements (\citeds{LDM-635}). QA use cases provide no further
requirements.

With the current design of the Gen 3 Middleware, each dataset
can be linked to provenance information such as input datasets,
pipeline definition, configurations, and software version
(e.g. DMS-MWST-REQ-0024 and DMS-MWBT-REQ-0096 in LDM-556).
QC metric values will be Butler datasets so they will be associated
with specific data IDs, and their provenance can be traced like
other datasets. In the Gen 3 Middleware design, Butler/SuperTask
framework is responsible of producing provenance.  For production
runs, the Data Backbone Services may store additional provenance,
for example from the Batch Production Services, besides the
Butler/SuperTask generated provenance.  In the QA use cases, the
primary source of provenance will be the Gen3 Butler/SuperTask
generated provenance.

Regarding provenance of the database records, the QAWG does not
place a strong requirement on per-source provenance.  The current
design is that each database record in the production database is
either ingested from a file, of which the full provenance is traced,
or from an uniquely identifiable execution.  This design does not
directly provide detailed per-source provenance, such as what exact
input images acually contribute to the measurement of a particular
source, but the full inputs that can contribute to the source.  The
provenance tracking of synthetic sources is also unclear.  The QAWG
thinks most low-level information can be uncovered through the drill
down system (\S\ref{sec:design:drill}).  Diagnostic information can
also be computed in the pipeline codes and stored as additional
columns.  We recommend the tooling to investigate the composition
of coadds be made in the drill down system and does not put this
requirement in the provenance system.

The QAWG notes that some high level aggregate data products that
are derived from provenance data can be useful in QA work.  For
example, the number of images that contribute to each coadd patch
can be obtained from provenance data.  However, the QAWG are not
immediately convinced that it's worth spending significant time
investigating what aggregate products need to be considered.

Derived from \S\ref{sec:design:debug}.

This section should note:

\begin{itemize}

  \item{That provenance is an immediate issue impacting QA work, so a solution
  is a priority;}

  \item{Some requirements as to the granularity at which provenance tracking
  is necessary for QA.}

\end{itemize}

\subsection{Documentation content updates}
\label{sec:comp:doc}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Clearer guidance on unit tests.}
  \item{Clearer guidance on code review, with requirements for test coverage
  etc.}

\end{itemize}

\subsection{Testing for documentation}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Examples.}

\end{itemize}

\subsection{CI system updates}
\label{sec:comp:ci}

Derived from \S\ref{sec:design:test}.

\assign{John}

\begin{itemize}

  \item{Test coverage.}
  \item{Tighter control of the environment.}
  \item{Better notifications.}
  \item{Better descriptions of which jobs do what.}
  \item{Clear description of what Developers are required to do before merging
  to master (see also \S\ref{sec:comp:doc}).}

\end{itemize}


\subsection{Metrics Dashboard / SQuaSH}

Derived from \S\ref{sec:design:test}.

\assign{Angelo}

Broadly as current SQuaSH, but to track:

\begin{itemize}

  \item{Code execution time.}
  \item{Test coverage?}
  \item{Notifications of regressions.}

\end{itemize}

\subsection{Standard format dataset package}
\label{sec:comp:dataset}

Derived from \S\ref{sec:design:test}.

\assign{Hsin-Fang}

The key considerations and motivations to standardize test datasets
in the construction phase include:

\begin{itemize}

\item{Developers would like low-friction access to test datasets.
A central location where developers can look up what has been curated
is desired.}

\item{Developers would like datasets at multiple scales and
representative of various data quality and observing conditions.
The properties of these datasets much be well understood.}

\item{Besides raw (unprocessed) data, intermediate and final data
products from various stages of pipeline processing are wanted.
They facilitiate testing of algorithms which are only relevant to
later parts of the pipeline or analysis codes without the need of
regenerating the products.}

\item{Datasets require continuing maintenance. Data products based
on a recent software release are usually wanted.}

\end{itemize}


The standard format of a dataset package is a ready-to-use Butler
repository and follows the format of a Butler repository as defined
in its corresponding obs package.  The format is configurable by
design, however, it is tied to the codes in the stack, so can change
from a software stack version to another.  Besides implementations
in the obs packages and Butler, other evolvement in the software
stack, such as handling of calibration data and reference catalog,
can also make a once-working repository incompatible.  Therefore,
maintenance is occassionally needed to ensure the usability of a
dataset package. \textbf{The QAWG recommends having a per-dataset product
owner.}\footnote{At the time of writing, our test datasets include
the following: afwdata, ap\_verify\_hits2015, testdata\_cfht,
testdata\_deblender, testdata\_decam, testdata\_jointcal,
testdata\_lsstSim, testdata\_subaru, qserv\_testdata,
validation\_data\_cfht.  validation\_data\_decam, validation\_data\_hsc,
/datasets/auxTel, /datasets/comCam, /datasets/ctio0m9, /datasets/lsstCam,
/datasets/decam /datasets/des\_sn, /datasets/hsc, /datasets/refcats,
/datasets/sdss, /datasets/gapon}
Product owners are responsible for ensuring that the content and
use cases of the datasets are well described and compatible with
recent stack versions.  The owner of a dataset can typically be the
team with immediate use cases and knowledge of its camera package.

The Obs Pkg WG (\jira{RFC-393}) is charged to re-design and refactor
the obs packages for maintainability and extensibility. We suggest
the Obs Pkg WG take into considerations in their design to mitigate
the close tie between a Butler repository and its obs package
implmentations, as well as adopt a common structure across different
cameras when possible.  After the refactoring, the obs packages
shall rarely change so the dataset format will be more stable.  The
QAWG recommends prioritise the Obs Pkg WG.

In some cases, a dataset pacakge may contain additional data that are not
tenable in the format of a Butler repository. We recommend following the
format as described in DM Developer Guide Common Dataset Organization
and Policy \footnote{\url{https://developer.lsst.io/services/datasets.html}}
and update the policy as needed.

As for the storage of test datasets, we consider any test dataset
package being either small or large, based on its size and use cases.
The QAWG's recommendations are as follows.

\begin{itemize}
  \item \textbf{Storage of small datasets}

  We consider small datasets as those smaller than around 100 GB
  and comfortably operatable as a Git LFS repository. They are
  carefully selected to meet specific use cases. The use cases of
  small datasets include

  \begin{itemize}
    \item{as input test data in CI jobs in the DM Jenkins system
          (\S\ref{sec:comp:ci});}
    \item{as example data in documentations, demos, and tutorials.}
  \end{itemize}

  To meet their needs, we recommend them

  \begin{itemize}
    \item{packaged as EUPS products;}
    \item{made publicly available on GitHub;}
    \item{stored as Git LFS repositories as needed;}
    \item{tagged their versions with the DM software releases;}
    \item{documented clearly its use cases and named product owner for each dataset;}
  \end{itemize}

  \item \textbf{Storage of large datasets}

  We consider large datasets as those larger than around 100 GB and
  hence transferring over network takes longer than an hour typically.
  They can contain edge cases that have not been identified to form
  specific small test datasets, or for use cases in which data
  volume is important.  We recommend them

  \begin{itemize}
    \item{made available on LSST development machines (currently on GPFS);}
    \item{usable and shared by team members;}
    \item{protected under a disaster recovery policy;}
    \item{documented clearly its use cases and named product owner for each dataset;}
  \end{itemize}

\end{itemize}



\subsection{Standard test package design}

Derived from \S\ref{sec:design:test}.

\assign{Hsin-Fang}

Currently, automatic continuous integration tests are performed via
multipla packages under two designs: (1) Scons-based execution, including
ci\_hsc and ci\_ctio0m9, and (2) exeuction through shell scripts
in validate\_drp.  Both ci\_hsc and validate\_drp are run in
Jenkins and triggered by timers every night (\S\ref{sec:comp:ci}).

It's QAWG's understanding that the validate\_drp scripts will
eventually replace ci\_hsc and ci\_ctio0m9, and a set of test scripts
will be run in a meta-package named lsst\_ci.  However, at the time
of writing, the validate\_drp scripts test only the single frame
processing step, while ci\_hsc exercises almost the entire end-to-end
DRP pipelines.  There has not been sufficient resources in implementing
further processing in validate\_drp.  Similarly, the lsst\_dm\_stack\_demo
repository should be converted into an EUPS product and a test
script added to lsst\_ci for execution (\jira{DM-14806}).

The QAWG recommends priority to unify the CI test package design and
finish the transition to validate\_drp. If such effort cannot
be allocated, documentations should be added to clearly describe
the status quo, and recommendations for developers during the
transition should be added to the Developer Guide (similar to
\S\ref{sec:comp:doc} and \S\ref{sec:comp:ci}). Before validate\_drp
can replace ci\_hsc and ci\_ctio0m9, the packages should be maintained.


Should address the union of lsst\_dm\_stack\_demo, ci\_hsc, validate\_drp use
cases.

