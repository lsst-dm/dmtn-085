\section{Design sketch}
\label{sec:design}

In this section, we summarize the issues identified and approaches suggested
by the groups described in \S\ref{sec:approach}. From these, we synethesize a
number of concrete actions---tools to be developed, documentation to be
provided, etc---we which recommend to the project in \S\ref{sec:comp}.

\subsection{Pipeline debugging}
\label{sec:design:debug}

\assign{John}

The group does not identify a single, over-arching tool or concept which would
solve the problem of developer productivity and happiness. Instead, we believe
that developer needs can be best addressed by making incremental improvements
to a number of core pieces of \gls{dm} technology and infrastructure which
team members regularly interact with. In particular, we identified the
following major ``pain points'' for developers:

\begin{itemize}

  \item{The pipeline debugging system, lsstDebug, is badly documented and
  awkward to use, and developers lack appropriate guidelines on how to use it
  effectively.}

  \item{Developers find it hard to know how to debug their code when it is
  running at scale. Issues include parsing logs when a large number of
  concurrent processes are running; inadequate documentation of the existing
  Slurm system; uncertainty about replacement of Slurm with a future workflow
  system; and difficulties in understanding the provenance of data products.}

  \item{The project has issued unclear guidance and inadequate documentation
  to developers about the appropriate tools to be used for visualizing data.
  This is most pronounced for image visualization\footnote{Developers have the
  impression they ``ought to'' be using Firefly, but there is much uncertainty
  around its suitability for the task and its future development direction.},
  but also applies to catalog data.}

  \item{Developers struggle to get access to data for running tests---both
  small and large scale---in a convenient form. Large repositories exist on
  the project \gls{gpfs} system, but it's unclear what they contain or how
  to effectively access them\footnote{A canonical example is the \gls{hsc}
  public data release: the volume of data is overwhelming for a developer who
  simply needs some representative \gls{hsc} data to test an algorithm.};
  smaller repositories exist on GitHub\footnote{e.g.
  \href{https://github.com/lsst/validation_data_hsc}{validation\_data\_hsc},
  \href{https://github.com/lsst/afwdata}{afwdata},
  \href{https://github.com/lsst/ap_verify_hits2015}{ap\_verify\_hits2015},
  etc.}, but they are inconsistent in structure, content and documentation:
  it's impossible for a developer to quickly identify data which is relevant
  to their use case, or to establish whether some particular reduction of the
  data is ``correct''. Instead, they rely on folklore and talking to peers to
  find data that ``worked for somebody else'', with (often) predictably
  frustrating results.}

\end{itemize}

To address these issues, the working group suggests the development of a
number of separate-but-related components. These include:

\begin{itemize}

  \item{An updated system for instrumenting running pipeline code;
  effectively, a revision of lsstDebug. This is developed further in
  \S\ref{sec:comp:debug}.}

  \item{A revised set of tooling for generating, aggregating and analyzing
  logs. This is developed further in \S\ref{sec:comp:log}.}

  \item{Revised documentation on interacting with the workload management
  system. This is developed further in \S\ref{sec:comp:workload}.}

  \item{Guidelines for the structure and maintenance of data repositories.
  This is developed further in \S\ref{sec:comp:dataset}.}

  \item{A clear roadmap for the development of visualization tools, and,
  derived from that, guidelines on how to apply those in the development of
  pipelines. This is developed further in \S\S\ref{sec:comp:image} \&
  \ref{sec:comp:vis}.}

\end{itemize}

In addition, the WG suggests that prioritization of developer-accessible
systems for tracking and understanding the provenance of pipeline outputs. For
additional comments on provenance from a QA perspective, refer to
\S\ref{sec:comp:provenance}

\subsection{Drill down}
\label{sec:design:drill}

Drill-down workflows center on the need to quickly and efficiently identify
data processing problems. Typically, these will be identified from
discrepancies identified at higher levels of summary and aggregation.

We therefore envisage a drill-down system which provides rapid retrieval of
relevant quantities (metric values, image cutouts, catalog overlays, etc.)
combined with readily-(re)configurable interactive plotting tools. This is
provided in a browser-based tool which enables the user to rapidly access
successive layers of detail on aggregated metrics (effectively
``de-aggregating'' them on demand), and ultimately enables the user to
seamlessly transition to an interactive analysis environment\footnote{i.e. a
Jupyter notebook} primed with the data under investagion.

We suggest that this system should be linked with a metric tracking system:
selected \glspl{aggregate metric} from successive comparable\footnote{i.e.
using the same input data, running with the same configuration} processing
should be tracked as a time series, with the user able to identify outliers
and rapidly switch to the drill-down system to investigate.

The system implementing these capabilities must be able to handle large
datasets---for example, an entire HSC public data release---quickly.
Furthermore, we emphasize the importance of ease-of-use for the QA analyst in
order to shorten debugging cycles.

\subsection{Datasets and test infrastructure}
\label{sec:design:test}

\assign{John}
