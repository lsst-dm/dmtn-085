\subsection{Pipeline debugging}
\label{sec:design:debug}

The group does not identify a single, over-arching tool or concept which would
solve the problem of developer productivity and happiness. Instead, we believe
that developer needs can be best addressed by making incremental improvements
to a number of core pieces of \gls{dm} technology and infrastructure which
team members regularly interact with. In particular, we identified the
following major ``pain points'' for developers:

\begin{itemize}

  \item{The pipeline debugging system, lsstDebug, is badly documented and
  awkward to use, and developers lack appropriate guidelines on how to use it
  effectively.}

  \item{Developers find it hard to know how to debug their code when it is
  running at scale. Issues include parsing logs when a large number of
  concurrent processes are running; inadequate documentation of the existing
  Slurm system; uncertainty about replacement of Slurm with a future workflow
  system; and difficulties in understanding the \gls{provenance} of data
  products.}

  \item{The project has issued unclear guidance and inadequate documentation
  to developers about the appropriate tools to be used for visualizing data.
  This is most pronounced for image visualization\footnote{Developers have the
  impression they ``ought to'' be using Firefly, but there is much uncertainty
  around its suitability for the task and its future development direction.},
  but also applies to catalog data.}

  \item{Developers struggle to get access to data for running tests --- both
  small and large scale --- in a convenient form. Large repositories exist on
  the project \gls{gpfs} system, but it's unclear what they contain or how
  to effectively access them\footnote{A canonical example is the \gls{hsc}
  public data release: the volume of data is overwhelming for a developer who
  simply needs some representative \gls{hsc} data to test an algorithm.};
  smaller repositories exist on GitHub\footnote{e.g.
  \href{https://github.com/lsst/validation_data_hsc}{validation\_data\_hsc},
  \href{https://github.com/lsst/afwdata}{afwdata},
  \href{https://github.com/lsst/ap_verify_hits2015}{ap\_verify\_hits2015},
  etc.}, but they are inconsistent in structure, content and documentation:
  it's impossible for a developer to quickly identify data which is relevant
  to their use case, or to establish whether some particular reduction of the
  data is ``correct''. Instead, they rely on folklore and talking to peers to
  find data that ``worked for somebody else'', with (often) predictably
  frustrating results.}

\end{itemize}

To address these issues, the working group suggests the development of a
number of separate-but-related components. These include:

\begin{itemize}

  \item{An updated system for instrumenting running pipeline code;
  effectively, a revision of lsstDebug. This is developed further in
  \S\ref{sec:comp:debug}.}

  \item{A revised set of tooling for generating, aggregating and analyzing
  logs. This is developed further in \S\ref{sec:comp:log}.}

  \item{Revised documentation on interacting with the workload management
  system. This is developed further in \S\ref{sec:comp:workload}.}

  \item{Guidelines for the structure and maintenance of data repositories.
  This is developed further in \S\ref{sec:comp:dataset}.}

  \item{A clear roadmap for the development of visualization tools, and,
  derived from that, guidelines on how to apply those in the development of
  pipelines. This is developed further in \S\ref{sec:comp:vis}.}

\end{itemize}

In addition, the WG suggests that prioritization of developer-accessible
systems for tracking and understanding the \gls{provenance} of pipeline
outputs. For additional comments on provenance from a QA perspective, refer to
\S\ref{sec:comp:provenance}
